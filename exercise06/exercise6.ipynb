{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6eb788",
   "metadata": {},
   "source": [
    "# Exercise 6- Latent Dirichlet Allocation\n",
    "\n",
    "In this exercise, we will formulate the generative model of Latent Dirichlet Allocation (LDA) and use Gibbs Sampling to derive the latent topics of documents.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        11.12.2022\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=34630)\n",
    "\n",
    "### Help\n",
    "In case you cannot solve a task, you can use the saved values within the `help` directory:\n",
    "- Load arrays with [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.load.html)\n",
    "```\n",
    "np.load('help/array_name.npy')\n",
    "```\n",
    "- Load functions, classes and other objects with [Dill](https://dill.readthedocs.io/en/latest/dill.html)\n",
    "```\n",
    "import dill\n",
    "with open('help/some_func.pkl', 'rb') as f:\n",
    "    func = dill.load(f)\n",
    "```\n",
    "\n",
    "to continue working on the other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f990d3",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We will use a dataset consisting of [titles from scientific papers](https://www.kaggle.com/datasets/blessondensil294/topic-modeling-for-research-articles).\n",
    "\n",
    "For a fast lookup, we convert the titles into lists of word indices based on a fixed vocabulary.\n",
    "\n",
    "For example with the vocabulary\n",
    "\n",
    "| idx | word |\n",
    "| :- | -: | \n",
    "| 0 | big |\n",
    "| 1 | sentence |\n",
    "| 2 | random |\n",
    "\n",
    "the sentence\n",
    "    \n",
    "    \"This is a random sentence\"\n",
    "    \n",
    "would be converted to \n",
    "    \n",
    "    [2, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e8127",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Load the articles from `titles.txt` and transform them into lists of word indices using the vocabulary from `vocab.txt`.\n",
    "Discard titles that have no words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f305a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# TODO: load titles and vocab\n",
    "with open(\"titles.txt\", 'r') as f:\n",
    "    titles = f.read().replace(\";\", \"\").splitlines()\n",
    "with open(\"vocab.txt\", 'r') as f:\n",
    "    vocab = f.read().replace(\";\", \"\").splitlines()\n",
    "\n",
    "# TODO: transform each document into list of indices\n",
    "vocab_id = {word: id for id, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def to_bag(title: str) -> list[int]:\n",
    "    # replace non-alphabetical characters with whitespace\n",
    "    title = re.sub(r\"[^a-zA-Z]\", \" \", title)\n",
    "    # strip whitespace left and right\n",
    "    title = title.strip()\n",
    "    # collapse all longer whitespace into 1 space\n",
    "    title = re.sub(r\"\\s+\", \" \", title)\n",
    "    # read the words\n",
    "    words = title.split(\" \")\n",
    "    # convert to ids (can become an empty list)\n",
    "    return [vocab_id[word] for word in words if word in vocab_id]\n",
    "\n",
    "\n",
    "# collect all bags where the bags aren't empty\n",
    "bags = [to_bag(title) for title in titles if len(to_bag(title)) != 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc4056",
   "metadata": {},
   "source": [
    "# LDA Topic Model\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a model of how documents are generated from unobserved topics.\n",
    "The goal is then to learn the hidden parameters of the model which essentially means to discover the topics of documents.\n",
    "\n",
    "The model has the following parameters:\n",
    "\n",
    "- $K$... number of topics\n",
    "- $D$... number of documents\n",
    "- $V$... size of vocabulary\n",
    "- $N_i$... number of words in document $i$\n",
    "- $z_{il}$... topic of $l$-th word of document $i$\n",
    "- $w_{il}$... $l$-th word of document $i$\n",
    "\n",
    "and is depicted in the following graphical model:\n",
    "<div>\n",
    "<img src=\"images/generative_model.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Sampling a corpus of documents is done with four steps:\n",
    "\n",
    "1. Sample the topic distribution for each document from a dirichlet distribution \n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_i\\sim\\mathcal{D}(\\alpha)\\text{, for }i=1,\\dots,D\n",
    "\\end{equation}\n",
    "\n",
    "2. Sample the word distribution for each topic from a dirichlet distribution \n",
    "\n",
    "\\begin{equation}\n",
    "\\varphi_j\\sim\\mathcal{D}(\\beta)\\text{, for }j=1,\\dots,K\n",
    "\\end{equation}\n",
    "\n",
    "3. Sample the topics for each word-position according to the document specific topic distribution\n",
    "\n",
    "\\begin{equation}\n",
    "z_{il}\\sim Cat(\\theta_i)\\text{, for }l=1,\\dots N_i\n",
    "\\end{equation}\n",
    "\n",
    "4. Sample the words for each word-position according to the topic specific word distribution\n",
    "\n",
    "\\begin{equation}\n",
    "w_{il}\\sim Cat(\\varphi_{z_{il}})\\text{, for }l=1,\\dots N_i\n",
    "\\end{equation}\n",
    "\n",
    "For further detail, please refer to the following videos:\n",
    "\n",
    "- intuitive guide [Part1](https://www.youtube.com/watch?v=T05t-SqKArY), [Part2](https://www.youtube.com/watch?v=BaM1uiCpj_E)\n",
    "- more technical [ML Lecture from TÃ¼bingen](https://www.youtube.com/watch?v=z2q7LhsnWNg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400d27d",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Implement this generative model and explore the influence of the dirichlet parameters $\\alpha$ and $\\beta$ on the topics of the document.\n",
    "\n",
    "For simplicity we assume that all documents have the same length $N$, and that we draw from the dirichlet distributions using the uniform vectors $[\\alpha]_{1,\\dots, D}$ and $[\\beta]_{1,\\dots, K}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "107e6a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha, beta: 0.10, 0.10\n",
      "topics probs:\n",
      "[[0.011 0.    0.004 0.985 0.   ]\n",
      " [0.    0.905 0.001 0.    0.094]\n",
      " [0.    0.012 0.    0.936 0.052]]\n",
      "alpha, beta: 1.00, 1.00\n",
      "topics probs:\n",
      "[[0.432 0.133 0.152 0.261 0.021]\n",
      " [0.311 0.257 0.164 0.149 0.119]\n",
      " [0.053 0.491 0.101 0.041 0.313]]\n",
      "alpha, beta: 0.50, 2.00\n",
      "topics probs:\n",
      "[[0.13  0.307 0.075 0.473 0.016]\n",
      " [0.44  0.179 0.024 0.    0.357]\n",
      " [0.02  0.001 0.098 0.125 0.756]]\n",
      "alpha, beta: 2.00, 0.50\n",
      "topics probs:\n",
      "[[0.356 0.303 0.129 0.124 0.088]\n",
      " [0.221 0.049 0.242 0.219 0.269]\n",
      " [0.1   0.293 0.177 0.158 0.272]]\n",
      "alpha, beta: 5.00, 5.00\n",
      "topics probs:\n",
      "[[0.199 0.263 0.183 0.226 0.129]\n",
      " [0.236 0.196 0.203 0.197 0.169]\n",
      " [0.15  0.187 0.181 0.35  0.133]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sample_LDA(alpha: float, beta: float, D: int, K: int, N: int, V: int) -> np.ndarray:\n",
    "    '''\n",
    "    @Params:\n",
    "        alpha... dirichlet prior for document-topic distribution\n",
    "        beta... dirichlet prior for topic-word distribution\n",
    "        D... number of documents\n",
    "        K... number of topics\n",
    "        N... number of words/document\n",
    "        V... size of vocabulary\n",
    "\n",
    "    @Returns:\n",
    "        samples from LDA model\n",
    "    '''\n",
    "\n",
    "    # turn alpha and beta into the respective vectors\n",
    "    alpha = np.full(K, alpha)\n",
    "    beta = np.full(V, beta)\n",
    "    # theta[d], d in [D]: probability distribution of topics for document d\n",
    "    # theta[d][t], d in [D], t in [K]: probability that document d is topic t\n",
    "    theta = np.array([np.random.dirichlet(alpha) for _ in range(D)])\n",
    "    # phi[t], t in [K]: probability distribution of words for topic t\n",
    "    # phi[t,w], t in [K], w in [V]: probability that topic t turns into word w\n",
    "    phi = np.array([np.random.dirichlet(beta) for _ in range(K)])\n",
    "    # z[d,n], d in [D], n in [N]: sampled topic in document d at position n\n",
    "    z = np.array([np.random.choice(K, size=N, p=theta[d]) for d in range(D)])\n",
    "    # w[d,n], d in [D], n in [N]: sampled word in document d at position n from topic distribution phi[t] where t = z[d][n] is the topic at that position\n",
    "    w = np.array([[np.random.choice(V, size=1, p=phi[z[d, n]]) for n in range(N)] for d in range(D)])\n",
    "    # return the documents filled with words\n",
    "    return theta, phi, z, w\n",
    "\n",
    "\n",
    "# TODO: observe statistics\n",
    "# n_documents = len(bags)\n",
    "# n_topics = 20\n",
    "# n_words_per_document = 10\n",
    "# vocab_size = len(vocab)\n",
    "n_documents = 3\n",
    "n_topics = 5\n",
    "n_words_per_document = 10\n",
    "vocab_size = 5\n",
    "\n",
    "priors = [\n",
    "    (0.1, 0.1),\n",
    "    (1, 1),\n",
    "    (0.5, 2),\n",
    "    (2, 0.5),\n",
    "    (5, 5)\n",
    "]\n",
    "\n",
    "# NOTE: low prior seems to promote sparsity\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    for alpha, beta in priors:\n",
    "        theta, phi, z, w = sample_LDA(alpha, beta, n_documents, n_topics, n_words_per_document, vocab_size)\n",
    "        print(f\"alpha, beta: {alpha:.2f}, {beta:.2f}\\ntopics probs:\\n{theta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da39859",
   "metadata": {},
   "source": [
    "# Parameter Learning\n",
    "\n",
    "In reality, we only know $W$ and we are interested in the latent matrices\n",
    "\\begin{equation}\n",
    "\\Theta = [\\theta_i]_{i=1,\\dots,D}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\Phi = [\\varphi_j]_{j=1,\\dots,T}\\,,\n",
    "\\end{equation}\n",
    "which hold the probabilities over topics for each document and the probabilities over words for each topic respectively.\n",
    "Both matrices are latent and controlled via latent Dirichlet-Priors, hence the name \"latent dirichlet allocation\".\n",
    "\n",
    "In other words we are interested in estimates \n",
    "\\begin{align}\n",
    "\\hat{\\Theta}&\\approx\\Theta\\\\\n",
    "\\hat{\\Phi}&\\approx\\Phi\\\\\n",
    "\\end{align}\n",
    "\n",
    "In practice, it is sufficient to get an estimate for the word - topic assignments $Z$.\n",
    "\n",
    "From $\\hat{Z}$ and $W$ one can then simply calculate the Maximum Likelihood estimates $\\hat{\\Theta}_{\\text{ML}}$ and $\\hat{\\Phi}_{\\text{ML}}$ for the categoricals.\n",
    "\n",
    "\n",
    "## Gibbs Sampling\n",
    "\n",
    "The standard way to get $\\hat{Z}$ is by estimating the posterior\n",
    "\\begin{equation}\n",
    "p(Z|W)\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Unfortunately this posterior is intractable to compute directly, so we have to fall back to sampling.\n",
    "\n",
    "Let \n",
    "- $W$ be the given word collection\n",
    "- $Z$ be an assignment of words to topics (collection of topics)\n",
    "- $d_i\\subseteq W$ be a specific document (collection of words)\n",
    "- $(d_i, w_i, z_i)$ be the triple that defines a specific word in a document with a topic\n",
    "\n",
    "[Gibbs Sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) in the form we want to use here consists of a simple loop:\n",
    "\n",
    "1. Create an initial $\\hat{Z}$\n",
    "2. For each word in each document $(d_i, w_i, z_i)$:\n",
    "   - resample $z_i$ from $p(z_i = j | Z\\setminus z_i, W)$\n",
    "   - update $\\hat{Z}$\n",
    "\n",
    "It turns out that\n",
    "\n",
    "\\begin{equation}\n",
    "p(z_i = j | Z\\setminus z_i, W)\\propto\\cfrac{|\\left\\{w\\in W: w=w_i\\wedge \\text{topic}(w)=j\\right\\}| + \\beta}{|\\left\\{w\\in W: \\text{topic}(w)=j\\right\\}| + V\\beta}\\cdot\\cfrac{|\\left\\{w\\in d_i: \\text{topic}(w)=j\\right\\}| + \\alpha}{|d_i| + K\\alpha}\\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6933de1c",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Implement the following class for Gibbs Sampling.\n",
    "\n",
    "The function `.sample` should return a dictionary that holds the intermediate results for each iteration (we need this later).\n",
    "\n",
    "Use the class to sample for 200 iterations with $K=5$ topics, and $\\alpha=\\beta=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "108b656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "class LDA_GibbsSampler:\n",
    "    def __init__(self, docs: list[list[int]], vocab: list[str], K: int, alpha: float, beta: float):\n",
    "        '''\n",
    "        @Params:\n",
    "            docs... list of lists of indices (see Task 1)\n",
    "            vocab... list of words (see Task 1)\n",
    "            K... number of topics\n",
    "        '''\n",
    "\n",
    "        self.docs = docs\n",
    "        self.vocab = vocab\n",
    "        self.word2idx = {v: i for i, v in enumerate(self.vocab)}\n",
    "        self.idx2word = {i: v for i, v in enumerate(self.vocab)}\n",
    "\n",
    "        self.D = len(self.docs)\n",
    "        self.V = len(self.word2idx)\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def sample(self, iterations: int, seed: int = 0) -> dict:\n",
    "        '''\n",
    "        Performs Gibbs Sampling for LDA topic model\n",
    "\n",
    "        @Params:\n",
    "            iterations... number of iterations\n",
    "            seed... random seed (for initialization)\n",
    "        @Returns:\n",
    "            dictionary with results from sampling process (key = iteration, value = results after iteration)\n",
    "        '''\n",
    "\n",
    "        # prepare word list\n",
    "        word_list = np.array([word for document in self.docs for word in document], dtype=int)\n",
    "        # we've got a topic for every word in every document\n",
    "        z_size = len(word_list)\n",
    "        # prepare respective documents\n",
    "        respective_docs = np.array([i for i, document in enumerate(self.docs) for word in document], dtype=int)\n",
    "\n",
    "        # seed it\n",
    "        np.random.seed(seed)\n",
    "        # create initial Z (a topic t in [K] for every word)\n",
    "        Z = np.random.choice(self.K, size=z_size)\n",
    "\n",
    "        # remember where that word occurs\n",
    "        # (w, i) => w == w_i\n",
    "        same_word = np.array([word_list == word for word in range(self.V)])\n",
    "        # remember where each document is\n",
    "        # (d, i) => d == d_i\n",
    "        same_document = np.array([respective_docs == document for document in range(self.D)])\n",
    "        # remember where each topics occurs\n",
    "        # (t, i) => t == z_i\n",
    "        same_topic = np.array([Z == topic for topic in range(self.K)])\n",
    "\n",
    "        # number of occurences of that word\n",
    "        n_occurences = np.array([np.count_nonzero(same_word[word]) for word in range(self.V)])\n",
    "        # remember how long each document is\n",
    "        document_length = np.array([np.count_nonzero(same_document[document]) for document in range(self.D)])\n",
    "        # (word, topic) -> number of occurences of the that word with that topic\n",
    "        n_occurences_with_topic = np.array([[np.count_nonzero(same_word[word] & same_topic[topic]) for topic in range(self.K)] for word in range(self.V)])\n",
    "        # (doc, topic) -> number of words in the document with that topic\n",
    "        n_occurences_of_topic_in_doc = np.array([[np.count_nonzero(same_document[document] & same_topic[topic]) for topic in range(self.K)] for document in range(self.D)])\n",
    "        # divisor never changes for any word/document combination\n",
    "        # (doc, word) => divisor\n",
    "        divisor = [[(n_occurences[word] + self.V * self.beta) * (document_length[document] + self.K * self.alpha) for word in range(self.V)] for document in range(self.D)]\n",
    "        # create the snapshot dict\n",
    "        snapshots: dict[int, np.ndarray] = {}\n",
    "        for iteration in range(iterations):\n",
    "            for i in range(z_size):\n",
    "                old_topic = Z[i]\n",
    "                word = word_list[i]\n",
    "                document = respective_docs[i]\n",
    "                # calculate the probability for every new topic j\n",
    "                divident = (n_occurences_with_topic[word] + self.beta) * (n_occurences_of_topic_in_doc[document] + self.alpha)\n",
    "                p = divident / divisor[document][word]\n",
    "                # normalize probability\n",
    "                p /= np.sum(p)\n",
    "                # sample from it and overwrite Z\n",
    "                new_topic = np.random.choice(self.K, p=p)\n",
    "                Z[i] = new_topic\n",
    "                # update the number of entries with the same word and topic\n",
    "                n_occurences_with_topic[word][old_topic] -= 1\n",
    "                n_occurences_with_topic[word][new_topic] += 1\n",
    "                # update the number of entries in the same document with the same topic\n",
    "                n_occurences_of_topic_in_doc[document][old_topic] -= 1\n",
    "                n_occurences_of_topic_in_doc[document][new_topic] += 1\n",
    "            snapshots[iteration] = Z.copy()\n",
    "        return snapshots\n",
    "\n",
    "\n",
    "# TODO: sample 1000 samples from p(Z|W)\n",
    "n_topics = 5\n",
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "sampler = LDA_GibbsSampler(bags, vocab, n_topics, alpha, beta)\n",
    "snapshots = sampler.sample(iterations=200)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6210df1",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "A good topic model should have two properties:\n",
    "\n",
    "1. Each document should have only a few topics\n",
    "2. Each topic should have only a few words\n",
    "\n",
    "Think about how to measure these two properties, track them over the sampling iterations and visualize this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdbf538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: track + visualize properties of topic assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7bfbe4",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "The result of our Gibbs Sampling process are samples from $p(Z|W)$.\n",
    "\n",
    "Remember that we want to have an estimate $\\hat{Z}$ and from there calculate $\\hat{\\Theta}_{\\text{ML}}$ and $\\hat{\\Phi}_{\\text{ML}}$.\n",
    "\n",
    "Based on these we can then answer all kinds of inference queries.\n",
    "\n",
    "### Task 5\n",
    "\n",
    "Use the topic assignments from the sampling process $Z^{(i)}$ to estimate \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{Z} = \\cfrac{1}{m} \\sum\\limits_{i=1}^m Z^{(i)} \\approx \\mathbb{E}_{p(Z|W)}Z\n",
    "\\end{equation}\n",
    "\n",
    "For the estimation, take every 10-th sample from the 100-th iteration onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: estimate Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51bc40c",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Think about what the Maximum Likelihood estimates $\\hat{\\Theta}_{\\text{ML}}$ and $\\hat{\\Phi}_{\\text{ML}}$ should be given $\\hat{Z}$.\n",
    "\n",
    "Use your estimate $\\hat{Z}$ to calculate $\\hat{\\Theta}_{\\text{ML}}$ and $\\hat{\\Phi}_{\\text{ML}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: estimate phi, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d3ab8",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5c69b",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "For each topic, what are the top three words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d6d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: top three words for topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8521f539",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "What is the topic distribution for the second title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288ec70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: topic distribution for second title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9cb030",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Select the top 10 titles that are most typical for topic 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809dc16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: top 10 titles for topic 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c06546d",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Visualize the soft clustering that is induced by the topic distribution vectors for each title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: visualize clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
