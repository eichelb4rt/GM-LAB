{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5435c02",
   "metadata": {},
   "source": [
    "# Exercise 4 - Tensor Networks\n",
    "In this exercise, we will inspect the canonical parameterization of a graphical model and calculate the normalization constant to answer inference queries.\n",
    "\n",
    "Later, we will compare the speed of calculating the normalization constant using different orders of tensor contractions.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        27.11.2022\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=34630)\n",
    "\n",
    "### Help\n",
    "In case you cannot solve a task, you can use the saved values within the `help` directory:\n",
    "- Load arrays with [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.load.html)\n",
    "```\n",
    "np.load('help/array_name.npy')\n",
    "```\n",
    "- Load functions with [Dill](https://dill.readthedocs.io/en/latest/dill.html)\n",
    "```\n",
    "import dill\n",
    "with open('help/some_func.pkl', 'rb') as f:\n",
    "    func = dill.load(f)\n",
    "```\n",
    "\n",
    "to continue working on the other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ed19d",
   "metadata": {},
   "source": [
    "## Graphical Models\n",
    "Let $p(x)$ be a multivariate categorical on the sample space $\\mathcal{X}$.\n",
    "In the canonical parameterization we define $p$ to be an exponentiated sum of interaction order parameters:\n",
    "\\begin{align}\n",
    "p(x) = \\exp\\left(q(x)\\right)\\,,\n",
    "\\end{align}\n",
    "where $q(x)$ is a sum of all possible interaction orders\n",
    "\\begin{align}\n",
    "q(x) = \\sum\\limits_{k=1}^n\\sum\\limits_{i=(i_1,\\dots,i_k)}q_i(x_{i_1}, \\dots, x_{i_k})\\,.\n",
    "\\end{align}\n",
    "In graphical models, we reduce the number of parameters by setting specific interactions $q_i$ to 0.\n",
    "\n",
    "This notation is a little confusing, so lets exercise trough a **concrete example**.\n",
    "\n",
    "Consider a multivariate categorical $p(x_0,x_1,x_2,x_3)$.\n",
    "Furthermore we restrict ourselves to unary and pairwise interaction orders (interactions of order >2 have been set to 0).\n",
    "\n",
    "This means, that we have single interaction parameter vectors $q_0, q_1, q_2, q_3$ and parwise interaction parameter matrices $q_{01}, q_{02}, q_{03}, q_{12}, q_{13}, q_{23}$.\n",
    "The $q_i$ hold the (unary) interaction parameters for $x_i$ and $q_{ij}$ holds the interaction parameters for $x_i$ and $x_j$.\n",
    "\n",
    "With these parameters, the canonical parameterization from above looks like this:\n",
    "\\begin{align}\n",
    "q(x = [v_0, v_1, v_2, v_3]^T) &=\\sum_{i=0}^3 q_i[v_i] + \\sum_{j=0, j > i}^3 q_{ij}[v_i, v_j]\\\\\n",
    "&=q_0[v_0] + q_1[v_1] + q_2[v_2] + q_3[v_3]\\\\\n",
    "&+q_{01}[v_0, v_1] + q_{02}[v_0, v_2] + q_{03}[v_0, v_3]\\\\\n",
    "&+q_{12}[v_1, v_2]+q_{13}[v_1, v_3]\\\\\n",
    "&+q_{23}[v_2, v_3]\\,.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "### Task 1\n",
    "\n",
    "Load $q_i$ and $q_ij$ from the pickeled files `q_i.p` and `q_ij.p` respectively.\n",
    "How large are the sample spaces for each $x_i$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d550119b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 50, 100, 10]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"q_i.p\", 'rb') as f:\n",
    "    q_i = pickle.load(f)\n",
    "with open(\"q_ij.p\", 'rb') as f:\n",
    "    q_ij = pickle.load(f)\n",
    "SAMPLE_SPACE_SIZES = [len(prob_table) for prob_table in q_i]\n",
    "SAMPLE_SPACE = lambda i: range(SAMPLE_SPACE_SIZES[i])\n",
    "N_VARS = len(SAMPLE_SPACE_SIZES)\n",
    "SAMPLE_SPACE_SIZES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355ca1b",
   "metadata": {},
   "source": [
    "## Normalization Constant\n",
    "\n",
    "Here we have unnormalized probabilities, so we need to calculate the normalization constant first\n",
    "\\begin{align}\n",
    "K &= \\sum_{x}p(x)\\\\\n",
    "&= \\sum_{x}\\exp\\left(q(x)\\right)\\\\\n",
    "&= \\sum_{x}\\prod_{i} \\exp(q_i[x_i])\\prod_{j > i} \\exp(q_{ij}[x_i, x_j])\\\\\n",
    "&= \\sum_{x}\\prod_{i} t_i[x_i]\\prod_{j > i} t_{ij}[x_i, x_j]\\,,\n",
    "\\end{align}\n",
    "where $t_i = \\exp(q_i)$ and $t_{ij} = \\exp(q_{ij})$ with the elementwise exponential function.\n",
    "\n",
    "### Task 2\n",
    "\n",
    "A straighforward way to calculate this constant is iterating over every $x$ and summing up the $p(x)$.\n",
    "\n",
    "Calculate $K$ using for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60f9ed8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159744720.16636336"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# calculate normalization constant\n",
    "\n",
    "pairwise_product = lambda x, i, t_ij: np.prod([t_ij[i][j][x[i], x[j]] for j in range(i + 1, N_VARS)])\n",
    "prob_unnormalized = lambda x, t_i, t_ij: np.prod([t_i[i][x[i]] * pairwise_product(x, i, t_ij) for i in range(N_VARS)])\n",
    "\n",
    "def norm_const_naive(t_i: list, t_ij: list) -> float:\n",
    "    '''\n",
    "    Calculates normalization constant by iterating over each x.\n",
    "\n",
    "    @Params:\n",
    "        t_i... unary interaction parameters (exponentiated)\n",
    "        t_ij... binary interaction parameters (exponentiated)\n",
    "\n",
    "    @Returns:\n",
    "        normalization constant\n",
    "    '''\n",
    "\n",
    "    norm = 0\n",
    "    sample_spaces = [SAMPLE_SPACE(i) for i in range(N_VARS)]\n",
    "    for x in itertools.product(*sample_spaces):\n",
    "        norm += prob_unnormalized(x, t_i, t_ij)\n",
    "    return norm\n",
    "\n",
    "\n",
    "t_i = [np.exp(param_table) for param_table in q_i]\n",
    "t_ij = [[np.exp(param_table) for param_table in param_tables] for param_tables in q_ij]\n",
    "norm = norm_const_naive(t_i, t_ij)\n",
    "norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd774af",
   "metadata": {},
   "source": [
    "## Inference Queries\n",
    "\n",
    "With this normalization constant, we can now actually calculate probabilities and answer inference queries.\n",
    "\n",
    "### Task 3\n",
    "Calculate the prior marginal \n",
    "\\begin{align}\n",
    "p(x_3)\\,.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d11f754e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17600392, 0.07294889, 0.08339296, 0.10914227, 0.07890277,\n",
       "       0.10590401, 0.06383303, 0.08919156, 0.07370976, 0.14697082])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prior_marginal(i, t_i, t_ij, norm):\n",
    "    marginal_probs = np.zeros(SAMPLE_SPACE_SIZES[i])\n",
    "    for x in itertools.product(*[SAMPLE_SPACE(j) for j in range(N_VARS)]):\n",
    "        marginal_probs[x[i]] += prob_unnormalized(x, t_i, t_ij) / norm\n",
    "    return marginal_probs\n",
    "        \n",
    "        \n",
    "p_x_3 = prior_marginal(3, t_i, t_ij, norm)\n",
    "p_x_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35fb1d",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Calculate the probability \n",
    "\\begin{equation}\n",
    "p(x_2>20)\\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10108aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8110435319468282"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_x_2 = prior_marginal(2, t_i, t_ij, norm)\n",
    "p_x_2_g20 = 0\n",
    "for x_2 in range(21, SAMPLE_SPACE_SIZES[2]):\n",
    "    p_x_2_g20 += p_x_2[x_2]\n",
    "p_x_2_g20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25959146",
   "metadata": {},
   "source": [
    "## Tensor Contraction\n",
    "Calculating $K$ by iterating over every $x$ is quite slow.\n",
    "Lets look at how we can speed up this calculation.\n",
    "\n",
    "We can rewrite the calculation of $K$ as\n",
    "\n",
    "\\begin{align}\n",
    "K &= \\sum_{x}p(x)\\\\\n",
    "&= \\sum_{x}\\prod_{i} \\exp(q_i[x_i])\\prod_{j > i} \\exp(q_{ij}[x_i, x_j])\\\\\n",
    "&= \\sum_{x}\\prod_{i} t_i[x_i]\\prod_{j > i} t_{ij}[x_i, x_j]\\\\\n",
    "&= \\sum_{v_0=1}^{n_0}\\sum_{v_1=1}^{n_1}\\sum_{v_2=1}^{n_2}\\sum_{v_3=1}^{n_3}\\prod_{i} t_i[v_i]\\prod_{j > i} t_{ij}[v_i, v_j]\\,.\n",
    "\\end{align}\n",
    "\n",
    "In this form, calculating the normalization constant boils down to a single tensor contraction. \n",
    "\n",
    "Since contracting tensors in numpy is implemented in C under the hood, we can expect a significant speedup.\n",
    "\n",
    "### Task 5\n",
    "Calculate the normalization constant using a **single** contraction using the [Einstein-Summation](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html).\n",
    "\n",
    "For a brief introduction into `einsum`, see [here](https://ajcr.net/Basic-guide-to-einsum/) and [here](https://medium.com/ibm-data-ai/einsum-an-easy-intuitive-way-to-write-tensor-operation-9e12b8a80570).\n",
    "\n",
    "Make sure that you result is correct by comparing the result to the naive implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "673b26c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i, j, k, l, ij, ik, il, jk, jl, kl ->\n"
     ]
    }
   ],
   "source": [
    "# the indices we want to sum and multiply over\n",
    "indices = list(range(N_VARS))\n",
    "pairwise_indices = [(i, j) for i in range(N_VARS) for j in range(i + 1, N_VARS)]\n",
    "# the names of the indices\n",
    "name = ['i', 'j', 'k', 'l']\n",
    "# contract over all variables, and multiply pairwise\n",
    "einsum_notation = \", \".join([name[i] for i in indices] + [name[i] + name[j] for i, j in pairwise_indices]) + \" ->\"\n",
    "einsum_args = [t_i[i] for i in indices] + [t_ij[i][j] for i, j in pairwise_indices]\n",
    "print(einsum_notation)\n",
    "norm_einsum = np.einsum(einsum_notation, *einsum_args)\n",
    "assert np.isclose(norm_einsum, norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4a4ff",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Compare the execution times of calculating $K$ the naive way vs. using `einsum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a58dd5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.33 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1min 46s ± 1min 5s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "26.9 ms ± 6.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# TODO: compare execution times\n",
    "%timeit norm_const_naive(t_i, t_ij)\n",
    "%timeit np.einsum(einsum_notation, *einsum_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6c533",
   "metadata": {},
   "source": [
    "## Contraction order\n",
    "\n",
    "We see that using contraction speeds up the calculation. This however is not the end of optimization:\\\n",
    "The order of contraction can be permutated, potentially reducing the number of calculations. Here we want to permutate the order in which the variables are marginalized out.\n",
    "\n",
    "For example for two variables $x_0, x_1$:\n",
    "\\begin{align}\n",
    "K &= \\sum_{v_0=1}^{n_0}\\sum_{v_1=1}^{n_1} t_0[v_0]t_1[v_1]t_{01}[v_0, v_1]\\\\\n",
    "(1) &= \\sum_{v_0=1}^{n_0}t_0[v_0]\\sum_{v_1=1}^{n_1}t_1[v_1]t_{01}[v_0, v_1]\\\\\n",
    "(2) &= \\sum_{v_1=1}^{n_1}t_1[v_1]\\sum_{v_0=1}^{n_0}t_0[v_1]t_{01}[v_0, v_1]\\\\\n",
    "\\end{align}\n",
    "\n",
    "Can be calculated as (1)\n",
    "1. Contracting $t_{01}$ and $t_{1}$ over the index $x_1$\n",
    "2. Contracting the result from 1. with $t_0$ over the index $x_0$\n",
    "\n",
    "or (2)\n",
    "1. Contracting $t_{01}$ and $t_{0}$ over the index of $x_0$\n",
    "2. Contracting the result from 1. with $t_1$ over the index of $x_1$\n",
    "\n",
    "Depending on the tensor dimensions, one calculation can be faster than the other.\n",
    "\n",
    "\n",
    "### Task 7\n",
    "\n",
    "Implement the following function that contracts the tensors in a given order.\n",
    "\n",
    "As an example for three variables, the order\n",
    "\n",
    "```\n",
    "['i', 'j', 'k']\n",
    "```\n",
    "\n",
    "with the tensor dictionary\n",
    "\n",
    "```\n",
    "tensor_dict = {\n",
    "'i' : t_i,\n",
    "'j' : t_j,\n",
    "'k' : t_k,\n",
    "'ij' : t_ij,\n",
    "'ik' : t_ik,\n",
    "'jk' : t_jk\n",
    "}\n",
    "```\n",
    "will perform the following contractions\n",
    "\n",
    "1. `tmp = np.einsum('i, ij, ik -> jk', t_i, t_ij, t_ik) # marginalize out i`\n",
    "2. `tmp = np.einsum('j, jk, jk -> k', t_j, t_jk, tmp) # marginalize out j`\n",
    "3. `tmp = np.einsum('k, k -> ', t_k, tmp) # marginalize out k`\n",
    "\n",
    "Make sure that the results are correct and compare the times of different marginalization orders to those from Task 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67c4c496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i, ij, il, ik -> jkl\n",
      "j, jl, jk, jkl -> kl\n",
      "l, kl, kl -> k\n",
      "k, k -> \n"
     ]
    }
   ],
   "source": [
    "def norm_const_order(order: list, tensor_dict: dict, show_notations=False) -> float:\n",
    "    '''\n",
    "    Calculates the normalization constant using tensor contraction with a specific order.\n",
    "\n",
    "    @Params:\n",
    "        order... list of variables in the order of their marginalization\n",
    "        tensor_dict... dict that stores which tensors are for which variable combination\n",
    "\n",
    "    @Returns:\n",
    "        normalization constant K\n",
    "\n",
    "    '''\n",
    "\n",
    "    for order_index, value_index in enumerate(order):\n",
    "        # what indices will be left after marginalization\n",
    "        remaining = order[order_index + 1:]\n",
    "        # indices should always be sorted in the notation\n",
    "        sum_over = [value_index] + [value_index + r if value_index < r else r + value_index for r in remaining]\n",
    "        sum_into = \"\".join(sorted(remaining))\n",
    "        einsum_args = [tensor_dict[value_indices] for value_indices in sum_over]\n",
    "        # if we're past the first marginalization, we have to include the last result\n",
    "        if order_index > 0:\n",
    "            last_sum_into = \"\".join(sorted([value_index] + remaining))\n",
    "            sum_over.append(last_sum_into)\n",
    "            einsum_args.append(contracted)\n",
    "        # build and execute einsum\n",
    "        einsum_notation = f\"{', '.join(sum_over)} -> {sum_into}\"\n",
    "        if show_notations:\n",
    "            print(einsum_notation)\n",
    "        contracted = np.einsum(einsum_notation, *einsum_args)\n",
    "    return contracted\n",
    "\n",
    "\n",
    "tensor_dict = {name[i]: t_i[i] for i in range(N_VARS)}\n",
    "tensor_dict |= {name[i] + name[j]: t_ij[i][j] for i, j in pairwise_indices}\n",
    "# order = [\"i\", \"j\", \"k\", \"l\"]\n",
    "order = [\"i\", \"j\", \"l\", \"k\"]\n",
    "norm_einsum_ordered = norm_const_order(order, tensor_dict, show_notations=True)\n",
    "assert np.isclose(norm_einsum_ordered, norm_einsum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d916bab",
   "metadata": {},
   "source": [
    "## Optimal contraction order\n",
    "\n",
    "We see that the contraction order has quite a lot of effect on the computation times.\n",
    "\n",
    "In fact, the problem of finding the best contraction order is generally NP-hard and an active area of research.\n",
    "In Python, the package [opt_einsum](https://optimized-einsum.readthedocs.io/en/stable/) provides heuristics to find an (near-)optimal contraction order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce1a59",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Use `opt_einsum` to calculate $K$, make sure result is correct.\n",
    "Again measure the execution time and compare to the other methods.\n",
    "\n",
    "Note: if you are interested, you can use `opt_einsum.contract_path` to have a look at the optimal contraction order that was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f97651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.59 ms ± 1.04 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "10.9 ms ± 337 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(1, 8), (1, 7), (0, 4), (0, 6), (0, 3), (3, 4), (0, 2), (1, 2), (0, 1)],\n",
       "   Complete contraction:  i,j,k,l,ij,ik,il,jk,jl,kl->\n",
       "          Naive scaling:  4\n",
       "      Optimized scaling:  4\n",
       "       Naive FLOP count:  7.500e+6\n",
       "   Optimized FLOP count:  1.542e+6\n",
       "    Theoretical speedup:  4.864e+0\n",
       "   Largest intermediate:  1.500e+4 elements\n",
       " --------------------------------------------------------------------------------\n",
       " scaling        BLAS                current                             remaining\n",
       " --------------------------------------------------------------------------------\n",
       "    2              0               jl,j->jl             i,k,l,ij,ik,il,jk,kl,jl->\n",
       "    2              0               kl,k->kl               i,l,ij,ik,il,jk,jl,kl->\n",
       "    2              0               il,i->il                 l,ij,ik,jk,jl,kl,il->\n",
       "    2              0               il,l->il                   ij,ik,jk,jl,kl,il->\n",
       "    3              0             jl,ij->jli                     ik,jk,kl,il,jli->\n",
       "    3              0            jli,il->jli                        ik,jk,kl,jli->\n",
       "    3              0             kl,ik->kli                          jk,jli,kli->\n",
       "    4           GEMM            kli,jli->kj                               jk,kj->\n",
       "    2     DOT/EINSUM                kj,jk->                                    ->)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: use opt_einsum to compute K\n",
    "import opt_einsum\n",
    "assert np.isclose(norm, opt_einsum.contract(einsum_notation, *einsum_args))\n",
    "\n",
    "# TODO: timing\n",
    "%timeit contract(einsum_notation, *einsum_args)\n",
    "%timeit norm_const_order(order, tensor_dict)\n",
    "opt_einsum.contract_path(einsum_notation, *einsum_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
