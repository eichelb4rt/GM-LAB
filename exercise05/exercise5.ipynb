{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5435c02",
   "metadata": {},
   "source": [
    "# Exercise 5 - Multivariate Gaussians\n",
    "\n",
    "In this exercise, we will estimate a Gaussian from a dataset and answer inference queries using the mean- and canonical parameterizations. Runtime experiments will illustrate the importance of both parameterizations.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        04.12.2022\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=34630)\n",
    "\n",
    "### Help\n",
    "In case you cannot solve a task, you can use the saved values within the `help` directory:\n",
    "- Load arrays with [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.load.html)\n",
    "```\n",
    "np.load('help/array_name.npy')\n",
    "```\n",
    "- Load functions with [Dill](https://dill.readthedocs.io/en/latest/dill.html)\n",
    "```\n",
    "import dill\n",
    "with open('help/some_func.pkl', 'rb') as f:\n",
    "    func = dill.load(f)\n",
    "```\n",
    "\n",
    "to continue working on the other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa48c929",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec52b2",
   "metadata": {},
   "source": [
    "In this exercise, we will use a dataset used for [predicting wine quality](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009).\n",
    "\n",
    "You find this dataset stored as `dataset.csv`. \n",
    "\n",
    "### Task 1\n",
    "Read this dataset into a $1599\\times 12$ matrix.\n",
    "\n",
    "Each row represents one specific wine, each column corresponds to a measured attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02642fed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1599, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load dataset into matrix\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "columns = list(df)\n",
    "dataset = df.to_numpy()\n",
    "\n",
    "print(columns)\n",
    "\n",
    "\n",
    "def indices(columns: list[str], *names: str) -> list[int]:\n",
    "    return [columns.index(name) for name in names]\n",
    "\n",
    "\n",
    "def columns_left(columns: list[str], indices_removed: list[int]) -> list[str]:\n",
    "    return [column for i, column in enumerate(columns) if i not in indices_removed]\n",
    "\n",
    "\n",
    "dataset.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07097c76",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "Here we use the model assumption that the samples come from a multivariate normal distribution. \n",
    "\n",
    "### Task 2\n",
    "Estimate the Maximum Likelihood parameters\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_{\\text{ML}} &= \\frac{1}{N}\\sum_{i=1}^Nx^{(i)}\\\\\n",
    "\\Sigma_{\\text{ML}} &= \\frac{1}{N}\\sum_{i=1}^N\\left(x^{(i)}-\\mu_{\\text{ML}}\\right)\\left(x^{(i)}-\\mu_{\\text{ML}}\\right)^T\n",
    "\\end{align}\n",
    "\n",
    "for a multivariate normal distribution based on this dataset. Here $N$ is the number of samples and $x^{(i)}$ is the i-th sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f84a7b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12,)\n",
      "(12, 12)\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate ML estimates\n",
    "import numpy as np\n",
    "\n",
    "N = dataset.shape[0]\n",
    "ml_mean = np.mean(dataset, axis=0)\n",
    "ml_cov = (N - 1) / N * np.cov(dataset.T)\n",
    "print(ml_mean.shape)\n",
    "print(ml_cov.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48177fa",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Now that we have estimated the parameters of our underlying model, we want to perform inference in order to answer the query:\n",
    "\n",
    "**\"What quality and alcohol level can we expect, if we observe a wine with**\n",
    "- **citric acid level of 0.6,**\n",
    "- **residual sugar of 2.5,**\n",
    "- **chlorides level of 0.1,**\n",
    "- **density of 0.994,**\n",
    "- **sulphate level of 0.5?\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213711cb",
   "metadata": {},
   "source": [
    "## Mean Parameterization\n",
    "\n",
    "The mean parameterization of a Gaussian consists of the mean vector $\\mu$ and the covariance matrix $\\Sigma$.\n",
    "\n",
    "**Marginalizing** dimensions from a Gaussian, to keep a subset $J$ of the dimensions results in a Gaussian with \n",
    "- Mean vector $\\mu_J$\n",
    "- Covariance matrix $\\Sigma_{JJ}$\n",
    "\n",
    "**Conditioning** a subset $J$ of the dimensions on values $x_J$ also gives us a Gaussian with \n",
    "- Mean vector $\\mu_I+\\Sigma_{IJ}\\Sigma_{JJ}^{-1}(x_J-\\mu_J)$ \n",
    "- Covariance matrix $S_{II} = \\Sigma_{II}-\\Sigma_{IJ}\\Sigma_{JJ}^{-1}\\Sigma_{JI}$\n",
    "\n",
    "Here, the subscripts indicate the selected dimensions of the variables. $I$ denotes the remaining dimensions, after we condition on the dimensions $J$. $S$ denotes the Schur complement.\n",
    "\n",
    "### Task 3\n",
    "Implement the following class of a Gaussian with mean parameterization. Then use your implementation to answer the query.\n",
    "\n",
    "Note: `marginalize` and `condition` should not return any parameters, but update the internal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5c17f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected quality: 6.101141437329366\n",
      "Expected alcohol level: 11.787708048356173\n"
     ]
    }
   ],
   "source": [
    "class MeanGaussian():\n",
    "    def __init__(self, mu, sigma):\n",
    "        '''\n",
    "        Mean parameterization of a gaussian\n",
    "\n",
    "        @Params: \n",
    "            mu... vector of size ndims\n",
    "            sigma... matrix of size ndims x ndims\n",
    "        '''\n",
    "\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def marginalize(self, idx_J):\n",
    "        '''\n",
    "        Marginalizes a set of indices from the Gaussian.\n",
    "\n",
    "        @Params:\n",
    "            idx_J... list of indices to keep after marginalization (these indices remain)\n",
    "\n",
    "        @Returns:\n",
    "            Nothing, parameters are changed internally\n",
    "        '''\n",
    "\n",
    "        self.mu = self.mu[idx_J]\n",
    "        self.sigma = self.sigma[idx_J, :][:, idx_J]\n",
    "\n",
    "    def condition(self, idx_J, x_J):\n",
    "        '''\n",
    "        Conditions a set of indices on values.\n",
    "\n",
    "        @Params:\n",
    "            idx_J... list of indices that are conditioned on\n",
    "            x_J... values that are conditioned on\n",
    "\n",
    "        @Returns:\n",
    "            Nothing, parameters are changed internally\n",
    "        '''\n",
    "\n",
    "        idx_I = [i for i in range(self.mu.shape[0]) if i not in idx_J]\n",
    "        sigma_ii = self.sigma[idx_I, :][:, idx_I]\n",
    "        sigma_ij = self.sigma[idx_I, :][:, idx_J]\n",
    "        sigma_jj_inv = np.linalg.inv(self.sigma[idx_J, :][:, idx_J])\n",
    "\n",
    "        self.mu = self.mu[idx_I] + sigma_ij @ sigma_jj_inv @ (x_J - self.mu[idx_J])\n",
    "        self.sigma = sigma_ii - sigma_ij @ sigma_jj_inv @ sigma_ij.T\n",
    "\n",
    "\n",
    "# TODO: answer query\n",
    "\n",
    "mean_gaussian = MeanGaussian(ml_mean, ml_cov)\n",
    "idx_J = indices(columns, \"citric acid\", \"residual sugar\", \"chlorides\", \"density\", \"sulphates\")\n",
    "x_J = [0.6, 2.5, 0.1, 0.994, 0.5]\n",
    "columns_after_marg = columns_left(columns, idx_J)\n",
    "mean_gaussian.condition(idx_J, x_J)\n",
    "idx_I = indices(columns_after_marg, \"quality\", \"alcohol\")\n",
    "mean_gaussian.marginalize(idx_I)\n",
    "print(f\"Expected quality: {mean_gaussian.mu[0]}\")\n",
    "print(f\"Expected alcohol level: {mean_gaussian.mu[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71f4d5",
   "metadata": {},
   "source": [
    "## Canonical Parameterization\n",
    "\n",
    "The canonical parameterization $(\\nu,\\Lambda)$ results from the mean parameterization trough\n",
    "\n",
    "\\begin{align}\n",
    "\\nu &=\\Sigma^{-1}\\mu\\\\\n",
    "\\Lambda &= \\Sigma^{-1}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "In the canonical parameterization, **marginalizing** dimensions from a Gaussian, to keep a subset $J$ of the dimensions, results in a Gaussian with \n",
    "- Vector $\\nu_J-\\Lambda_{IJ}\\Lambda_{JJ}^{-1}\\nu_J$\n",
    "- Precision matrix $S_{JJ}=\\Lambda_{JJ}-\\Lambda_{JI}\\Lambda_{II}^{-1}\\Lambda_{IJ}$\n",
    "\n",
    "**Conditioning** a subset $J$ of the dimensions on values $x_J$ again gives us a Gaussian with \n",
    "- Vector $\\nu_I-\\Lambda_{IJ}x_J$ \n",
    "- Precision matrix $\\Lambda_{II}$\n",
    "\n",
    "The subscripts indicate the selected dimensions of the variables. $I$ denotes the remaining dimensions, after we remove the dimensions $J$. $S$ denotes the Schur complement.\n",
    "\n",
    "We shall later see, that there are some cases, where you would prefer canonical parameterization over the mean parameterization.\n",
    "\n",
    "### Task 4\n",
    "Implement the following class of a Gaussian with canonical parameterization. Then use your implementation to answer the query.\n",
    "\n",
    "Note: `marginalize` and `condition` should not return any parameters, but update the internal parameters.\n",
    "The solution should be the same as in Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f78821f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected quality: 6.101141437329399\n",
      "Expected alcohol level: 11.787708048356278\n"
     ]
    }
   ],
   "source": [
    "class CanonicalGaussian():\n",
    "    def __init__(self, nu, lamb):\n",
    "        '''\n",
    "        Canconical representation of a gaussian\n",
    "\n",
    "        @Params: \n",
    "            nu... vector of size ndims\n",
    "            lamb... matrix of size ndims x ndims (precision matrix)\n",
    "        '''\n",
    "\n",
    "        self.nu = nu\n",
    "        self.lamb = lamb\n",
    "\n",
    "    def marginalize(self, idx_J):\n",
    "        '''\n",
    "        Marginalizes a set of indices from the Gaussian.\n",
    "\n",
    "        @Params:\n",
    "            idx_J... list of indices to keep after marginalization (these indices remain)\n",
    "\n",
    "        @Returns:\n",
    "            Nothing, parameters are changed internally\n",
    "        '''\n",
    "\n",
    "        idx_I = [i for i in range(self.nu.shape[0]) if i not in idx_J]\n",
    "        lamb_ji = self.lamb[idx_J, :][:, idx_I]\n",
    "        lamb_ii_inv = np.linalg.inv(self.lamb[idx_I, :][:, idx_I])\n",
    "        lamb_jj = self.lamb[idx_J, :][:, idx_J]\n",
    "\n",
    "        self.nu = self.nu[idx_J] - lamb_ji @ lamb_ii_inv @ self.nu[idx_I]\n",
    "        self.lamb = lamb_jj - lamb_ji @ lamb_ii_inv @ lamb_ji.T\n",
    "\n",
    "    def condition(self, idx_J, x_J):\n",
    "        '''\n",
    "        Conditions a set of indices on values.\n",
    "\n",
    "        @Params:\n",
    "            idx_J... list of indices that are conditioned on\n",
    "            x_J... values that are conditioned on\n",
    "\n",
    "        @Returns:\n",
    "            Nothing, parameters are changed internally\n",
    "        '''\n",
    "\n",
    "        idx_I = [i for i in range(self.nu.shape[0]) if i not in idx_J]\n",
    "        lamb_ij = self.lamb[idx_I, :][:, idx_J]\n",
    "        self.nu = self.nu[idx_I] - lamb_ij @ x_J\n",
    "        self.lamb = self.lamb[idx_I, :][:, idx_I]\n",
    "\n",
    "\n",
    "\n",
    "# TODO: answer query\n",
    "lamb = np.linalg.inv(ml_cov)\n",
    "nu = lamb @ ml_mean\n",
    "canonical_gaussian = CanonicalGaussian(nu, lamb)\n",
    "canonical_gaussian.condition(idx_J, x_J)\n",
    "canonical_gaussian.marginalize(idx_I)\n",
    "marg_sigma = np.linalg.inv(canonical_gaussian.lamb)\n",
    "marg_mean = marg_sigma @ canonical_gaussian.nu\n",
    "print(f\"Expected quality: {marg_mean[0]}\")\n",
    "print(f\"Expected alcohol level: {marg_mean[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d226f",
   "metadata": {},
   "source": [
    "# Computational costs\n",
    "\n",
    "Why do we need two different parameterizations of the same probability distribution?\n",
    "What is the difference?\n",
    "\n",
    "We cannot observe the effect of a different parameterization on our dataset, as it is way to small (too few dimensions).\n",
    "\n",
    "In the `synthetic/` directory, you find parameters for a Gaussian with 300 dimensions, as well as a value vector `x` for conditioning.\n",
    "Load these arrays and calculate the parameters for the canoncial parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31ed39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load synthetic parameters\n",
    "mu = np.load(\"synthetic/mu.npy\")\n",
    "sigma = np.load(\"synthetic/sigma.npy\")\n",
    "x_J = np.load(\"synthetic/x.npy\")\n",
    "lamb = np.linalg.inv(sigma)\n",
    "nu = lamb @ mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70f527",
   "metadata": {},
   "source": [
    "We now want to investigate the computation times for the following inference operations:\n",
    "\n",
    "1. Marginalize out the dimensions 200-299, then condition on the dimensions 100-199 with $x$\n",
    "2. Condition on the dimensions 100-199 with $x$, then marginalize out the dimensions 200-299\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"images/indices.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "Both operations yield the same result, $p(x_0,\\dots,x_{99}|x_{100},\\dots,x_{199})$ they just change the order of marginalization and conditioning.\n",
    "\n",
    "### Task 5\n",
    "Track the computational costs for both inference operations using the mean parameters and the canoncial parameters.\n",
    "\n",
    "What do you observe? Try to find an explanation for your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d50b723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canonical Parameterization times:\n",
      "marg first: 0.00699358800000482\n",
      "cond first: 0.005307515499989677\n",
      "Mean Parameterization times:\n",
      "marg first: 0.0034045405000142637\n",
      "cond first: 0.010268308999984583\n"
     ]
    }
   ],
   "source": [
    "# TODO: measure execution costs + explain observations\n",
    "import timeit\n",
    "\n",
    "# t_mean = timeit.timeit(lambda: )\n",
    "# keep 0 - 199\n",
    "idx_I_first_marg = list(range(0, 200))\n",
    "# condition on 100 - 199\n",
    "idx_J_first_marg = list(range(100, 200))\n",
    "# condition on 100 - 199\n",
    "idx_J_first_cond = list(range(100, 200))\n",
    "# keep 0 - 99\n",
    "idx_I_first_cond = list(range(0, 100))\n",
    "\n",
    "def canonical_marg_first():\n",
    "    gaussian = CanonicalGaussian(nu, lamb)\n",
    "    gaussian.marginalize(idx_I_first_marg)\n",
    "    gaussian.condition(idx_J_first_marg, x_J)\n",
    "\n",
    "def canonical_cond_first():\n",
    "    gaussian = CanonicalGaussian(nu, lamb)\n",
    "    gaussian.condition(idx_J_first_cond, x_J)\n",
    "    gaussian.marginalize(idx_I_first_cond)\n",
    "\n",
    "def mean_marg_first():\n",
    "    gaussian = MeanGaussian(mu, sigma)\n",
    "    gaussian.marginalize(idx_I_first_marg)\n",
    "    gaussian.condition(idx_J_first_marg, x_J)\n",
    "\n",
    "def mean_cond_first():\n",
    "    gaussian = MeanGaussian(mu, sigma)\n",
    "    gaussian.condition(idx_J_first_cond, x_J)\n",
    "    gaussian.marginalize(idx_I_first_cond)\n",
    "\n",
    "N = 200\n",
    "t_canonical_marg_first = timeit.timeit(canonical_marg_first, number=N) / N\n",
    "t_canonical_cond_first = timeit.timeit(canonical_cond_first, number=N) / N\n",
    "t_mean_marg_first = timeit.timeit(mean_marg_first, number=N) / N\n",
    "t_mean_cond_first = timeit.timeit(mean_cond_first, number=N) / N\n",
    "\n",
    "print(\"Canonical Parameterization times:\")\n",
    "print(f\"marg first: {t_canonical_marg_first}\")\n",
    "print(f\"cond first: {t_canonical_cond_first}\")\n",
    "print(\"Mean Parameterization times:\")\n",
    "print(f\"marg first: {t_mean_marg_first}\")\n",
    "print(f\"cond first: {t_mean_cond_first}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
